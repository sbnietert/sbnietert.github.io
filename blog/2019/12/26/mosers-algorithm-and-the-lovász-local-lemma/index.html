<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.62.2"><title>Moser's Algorithm and the Lovász Local Lemma - Sloan Nietert</title><meta property="og:title" content="Moser's Algorithm and the Lovász Local Lemma - Sloan Nietert"><link rel=stylesheet href=../../../../css/fonts.css media=all><link rel=stylesheet href=../../../../css/main.css media=all><link href="https://fonts.googleapis.com/css?family=Lato|Merriweather:400,700&display=swap&subset=latin-ext" rel=stylesheet><link href=../../../../css/boxes.css rel=stylesheet></head><body><div class=wrapper><header class=header><nav class=nav><a href=../../../../ class=nav-logo><img src=../../../../images/logo.png width=50 height=50 alt=SN></a><ul class=nav-links><li><a href=../../../../>Blog</a></li><li><a href=https://www.cs.cornell.edu/~nietert/>About Me</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title>Moser's Algorithm and the Lovász Local Lemma</h1><span class=article-author>Sloan Nietert</span>
<span class=article-date>2019-12-26</span><div class=article-content><p>In this survey, we’ll examine the <a href=https://en.wikipedia.org/wiki/Lov%C3%A1sz_local_lemma>Lovász Local Lemma</a> (LLL), a cornerstone of the probabilistic method, through the lens of theoretical computer science. This powerful tool was developed in the 70s by Hungarian mathematicians Lovász and Erdős to guarantee the existence of certain combinatorial structures<a href=#fn1 class=footnote-ref id=fnref1><sup>1</sup></a>, and it has since been borrowed by computer scientists to prove results about satisfiability problems, graph colorings, tree codes, integer programming, and much more<a href=#fn2 class=footnote-ref id=fnref2><sup>2</sup></a>.</p><p>For quite some time, this method remained entirely non-constructive, until a quest to constructify the LLL began in the 90s<a href=#fn3 class=footnote-ref id=fnref3><sup>3</sup></a>. For a while, all results required rather significant assumptions, but breakthrough work by Moser and Tardos in 2009 proved the efficacy of a suprisingly simple randomized search algorithm<a href=#fn4 class=footnote-ref id=fnref4><sup>4</sup></a> <a href=#fn5 class=footnote-ref id=fnref5><sup>5</sup></a>. This post will describe an elegant proof of Moser’s algorithm using the method of entropy compression. If you are already familiar with the LLL, you may want to skip to the <a href=#variable-lll-and-mosers-algorithm>third section</a>.</p><div id=the-probabilistic-method class="section level2"><h2>The Probabilistic Method</h2><p>The probabilistic method is a useful non-constructive technique that was pioneered by Paul Erdős in the 40s to prove foundational results in Ramsey theory<a href=#fn6 class=footnote-ref id=fnref6><sup>6</sup></a>. In general, this setup begins with a search space <span class="math inline">\(\Omega\)</span> (usually large and finite) and a set <span class="math inline">\(\mathcal{B}\)</span> of flaws that we aim to avoid. Specifically, each <span class="math inline">\(B \in \mathcal{B}\)</span> is a subset of <span class="math inline">\(\Omega\)</span> whose objects share some undesired trait, and we are searching for a flawless object <span class="math inline">\(\sigma \in \Omega \setminus \cup_{B \in \mathcal{B}} B\)</span>. The probabilistic approach to this search is to place a probability measure on <span class="math inline">\(\Omega\)</span>, so that <span class="math inline">\(\mathcal{B}\)</span> corresponds to a set of “bad” events. Then, we can borrow tools from probability theory to prove that the probability of no bad events occuring,
<span class="math display">\[
\Pr[\land_{B \in \mathcal{B}} \overline{B}] = 1 - \Pr[\lor_{B \in \mathcal{B}} B],
\]</span>
is strictly positive, certifying the existence of a flawless object. While this approach is often just a counting argument in disguise, it has proven to be quite fruitful and has helped combinatorialists avoid redeveloping existing probabilistic machinery. The classic applications of this method are to graph coloring, but <span class="math inline">\(k\)</span>-SAT seems better suited to a computer science audience.</p><div class=example><p>Let <span class="math inline">\(\varphi\)</span> be a Boolean formula in conjunctive normal form (CNF) with <span class="math inline">\(n\)</span> variables, <span class="math inline">\(m\)</span> clauses, and <span class="math inline">\(k\)</span> variables in each clause. The <span class="math inline">\(k\)</span>-SAT decision problem asks, given such a <span class="math inline">\(\varphi\)</span>, whether there exists an assignment of variables satisfying all of its clauses. We will diverge slightly from the traditional <span class="math inline">\(k\)</span>-SAT definition by requiring that the variables of each clause are distinct. In this case, our search space
<span class="math display">\[
\Omega = \{\text{True}, \text{False}\}^n
\]</span>
consists of variable assignments, and our flaws
<span class="math display">\[
B_i = \{ \sigma \in \Omega \mid \text{$\sigma$ violates clause $i$ of $\varphi$} \}, \quad i = 1, \dots, m
\]</span>
correspond to violated clauses. We will simply select the uniform probability measure over <span class="math inline">\(\Omega\)</span>, i.e. flip a fair coin to determine the assignment of each variable. With this choice,
<span class="math display">\[
\Pr[B_i] = \frac{1}{2^k},
\]</span>
since there is a single assignment of the <span class="math inline">\(k\)</span> variables in a CNF clause which cause it to be violated.</p></div><p>Perhaps the most generic tool we have at our disposal is the union bound. In the case of <span class="math inline">\(k\)</span>-SAT, this guarantees that a satisfying assignment exists when the number of clauses is small. Specifically, if <span class="math inline">\(m &lt; 2^k\)</span>, we have
<span class="math display">\[
\Pr[\land_{i=1}^m \overline{B_i}] = 1 - \Pr[\lor_{i=1}^m B_i] \geq 1 - \sum_{i=1}^m \Pr[B_i] = 1 - m 2^{-k} > 0,
\]</span>
as desired. Another simple case is when all of the clauses are disjoint; then, the <span class="math inline">\(B_i\)</span> events are independent and
<span class="math display">\[
\Pr[\land_{i=1}^m \overline{B_i}] = \prod_{i=1}^m \Pr[\overline{B_i}] = (1 - 2^{-k})^m > 0.
\]</span>
In general, we have the following when each bad event occurs with at most probability <span class="math inline">\(p\)</span>.</p><table><thead><tr class=header><th>Method</th><th>Assumptions on …</th><th>Sufficient condition for flawless object</th></tr></thead><tbody><tr class=odd><td>Union bound</td><td>n/a</td><td><span class="math inline">\(p &lt; 1/n\)</span></td></tr><tr class=even><td>Independence</td><td>Global event structure</td><td><span class="math inline">\(p &lt; 1\)</span></td></tr><tr class=odd><td>???</td><td>Local event structure</td><td><span class="math inline">\(p &lt; \,?\)</span></td></tr></tbody></table><p>As the final row suggests, it is natural to ask whether one can interpolate between these two results with assumptions on <em>local</em> event structure.</p></div><div id=the-lovász-local-lemma class="section level2"><h2>The Lovász Local Lemma</h2><p>This question was given an affirmative answer in the 70s when Lászlo Lovász developed his local lemma to establish a hypergraph colorability result<a href=#fn7 class=footnote-ref id=fnref7><sup>7</sup></a>. We will first describe the so-called symmetric LLL, which is often applied when the bad events are identical up to some (measure-preserving) permutation of the objects.</p><div class=Theorem><h3 id=symmetric-lll>Theorem (Symmetric LLL)</h3><p>Let <span class="math inline">\(\mathcal{B}\)</span> be a set of events such that each occurs with probability at most <span class="math inline">\(p\)</span> and is mutually independent from all but at most <span class="math inline">\(d\)</span> other events. Then, if <span class="math inline">\(ep(d + 1) \leq 1\)</span>, <span class="math inline">\(\Pr[\land_{B \in \mathcal{B}} \overline{B}] > 0\)</span>.</p></div><p>For <span class="math inline">\(d \geq 1\)</span>, this condition was later relaxed by Shearer to <span class="math inline">\(epd \leq 1\)</span><a href=#fn8 class=footnote-ref id=fnref8><sup>8</sup></a>. The dependency bound <span class="math inline">\(d\)</span> serves as a characterization of local event structure, and this result fits into table as <span class="math inline">\(p &lt; 1/(ed)\)</span>, nearly interpolating smoothly between independence and the union bound. Let’s examine the consequences of the LLL for <span class="math inline">\(k\)</span>-SAT.</p><div class=Example><h3>Example</h3><p>Suppose that each variable of our <span class="math inline">\(k\)</span>-CNF formula appears in at most <span class="math inline">\(2^k/(ke)\)</span> clauses. Then each clause can share variables with at most <span class="math inline">\(k \cdot 2^k/(ke) = 2^k/e\)</span> clauses, giving a dependency bound of <span class="math inline">\(d = 2^k/e\)</span> between our bad events. Since <span class="math inline">\(p = 2^{-k}\)</span>, <span class="math inline">\(epd = 1\)</span>, and the LLL condition (barely) holds, guaranteeing the existence of a satisfying variable assignment.</p></div><p>In the asymmetric case, we will require a more nuanced description of event structure.</p><div class=Definition><h3 id=dependency-graph>Definition (Dependency Graph)</h3><p>Let <span class="math inline">\(\mathcal{E}\)</span> be a set of events in a probability space. A graph with vertex set <span class="math inline">\(\mathcal{E}\)</span> is called a <strong>dependency graph</strong> for <span class="math inline">\(\mathcal{E}\)</span> if each event is mutually independent from all but its neighbors.</p></div><div class=Example><h3>Example</h3><p>Consider the 2-CNF formula <span class="math inline">\(\varphi = (x_1 \lor x_2) \land (\neg x_2 \lor x_3) \land (x_3 \lor \neg x_4)\)</span>. Then, we have three bad events to avoid; <span class="math inline">\(B_1\)</span> associated with <span class="math inline">\(x_1 \lor x_2\)</span>, <span class="math inline">\(B_2\)</span> associated with <span class="math inline">\(\neg x_2 \lor x_3\)</span>, and <span class="math inline">\(B_3\)</span> corresponding to <span class="math inline">\((x_3 \lor \neg x_4)\)</span>. The following are valid dependency graphs for this event set.
<img src=../../../../post/2019-12-26_lll_files/dependency-graphs.png alt="Example dependency graphs."></p></div><div class=Warning><h3 id=remark>Remark</h3><p>In the case of <span class="math inline">\(k\)</span>-SAT, there is a unique edge-minimum dependency graph with edges connecting events whose corresponding clauses share variables; however, there is no canonical dependency graph for general event sets. Furthermore, when we move from an event set to a dependency graph, we lose information about the events.</p></div><p>Now, we can state the LLL in its general form, where we let <span class="math inline">\(N_G(v)\)</span> denote the neighbors of vertex <span class="math inline">\(v\)</span> in graph <span class="math inline">\(G\)</span>.</p><div class=Theorem><h3 id=asymmetric-lll>Theorem (Asymmetric LLL)</h3><p>Given a set of bad events <span class="math inline">\(\mathcal{B}\)</span> with dependency graph <span class="math inline">\(G\)</span>, if there exists some <span class="math inline">\(x \in (0,1)^{\mathcal{B}}\)</span> such that
<span class="math display">\[
\Pr[B] \leq x_B \prod_{C \in N_G(B)} (1 - x_C) \quad \forall B \in \mathcal{B},
\]</span>
then <span class="math inline">\(\Pr[\land_{B \in \mathcal{B}} \overline{B}] > 0\)</span>.</p></div><div class=Remark><h3>Remark</h3><p>We can recover the symmetric case as a corollary by setting each <span class="math inline">\(x_B = 1/(d+1)\)</span>. Indeed, this choice of <span class="math inline">\(x\)</span> results in the requirement
<span class="math display">\[
\Pr[B] \leq \frac{1}{d+1}\left(1 - \frac{1}{d+1}\right)^d,
\]</span>
which is implied by the condition <span class="math inline">\(ep(d+1) \leq 1\)</span>, since
<span class="math display">\[
\frac{1}{e} \leq \left( 1 - \frac{1}{d+1} \right)^d.
\]</span></p></div><p>We omit a proof of the asymmetric LLL, though it essentially follows from inductive applications of Bayes’ Theorem.</p></div><div id=variable-lll-and-mosers-algorithm class="section level2"><h2>Variable LLL and Moser’s Algorithm</h2><p>In most applications of the LLL (for instance, <span class="math inline">\(k\)</span>-SAT and graph coloring), the bad events of interest are generated by a finite set of independent random variables. To capture this richer structure, we introduce the following.</p><div class=Definition><h3 id=event-variable-bigraph>Definition (Event-Variable Bigraph)</h3><p>Let <span class="math inline">\(\mathcal{B}\)</span> be a set of events and <span class="math inline">\(\mathcal{X}\)</span> be a set of mutually independent random variables such that each event <span class="math inline">\(B \in \mathcal{B}\)</span> is completely determined by a subset of variables <span class="math inline">\(\operatorname{vbl}(B) \subset \mathcal{X}\)</span>. We define the <strong>event-variable bigraph</strong> of this system to be the bipartite graph <span class="math inline">\(H = (\mathcal{B} \cup \mathcal{X}, E)\)</span> with <span class="math inline">\(B,X \in E\)</span> if and only if <span class="math inline">\(X \in \operatorname{vbl}(B)\)</span>.</p></div><div class=Definition><h3 id=base-dependency-graph>Definition (Base Dependency Graph)</h3><p>Given such an event-variable bigraph <span class="math inline">\(H = (\mathcal{B} \cup \mathcal{X}, E)\)</span>, we define its <strong>base dependency graph</strong> <span class="math inline">\(G_H = (\mathcal{B}, E')\)</span>, where <span class="math inline">\(B,B' \in E'\)</span> if and only if <span class="math inline">\(\operatorname{vbl}(B) \cap \operatorname{vbl}(B') \neq \emptyset\)</span>.</p></div><p>Observe that <span class="math inline">\(G_H\)</span> is a dependency graph for <span class="math inline">\(\mathcal{B}\)</span> (in fact, its edge-minimum dependency graph as long as no variable set is unecessarily large).</p><div class=Example><h3>Example</h3><p>Recall the previous the 2-CNF formula <span class="math inline">\(\varphi = (x_1 \lor x_2) \land (\neg x_2 \lor x_3) \land (x_3 \lor \neg x_4)\)</span> with corresponding bad events <span class="math inline">\(B_1, B_2, B_3\)</span>. This system admits the following event-variable bigraph,
<img src=../../../../post/2019-12-26_lll_files/event-variable-bigraph.png alt="Example dependency graphs."><br>where <span class="math inline">\(X_i\)</span> is the random variable corresponding to the assignment of <span class="math inline">\(x_i\)</span>.</p></div><p>With this variable formulation well-defined, we are now prepared to constructify the LLL. The algorithm which I will present was published by Robin Moser in 2008<a href=#fn9 class=footnote-ref id=fnref9><sup>9</sup></a> and extended to the general variable setting in 2009 alongside his advisor Gábor Tardos<a href=#fn10 class=footnote-ref id=fnref10><sup>10</sup></a>. Their approach was a major breakthrough over previous work, which required significant assumptions on top of this variable structure. Particularly suprising was its simplicity; the algorithm is perhaps the simplest non-trivial procedure one could imagine to find a satisfying variable assignment. Specifically, given a set <span class="math inline">\(\mathcal{B}\)</span> of bad events generated by a finite set of random variables <span class="math inline">\(\mathcal{X}\)</span>, Moser’s algorithm finds an assignment of the random variables such that all of the bad events are avoided, as follows.</p><div class=Algorithm><h3 id=resample>Algorithm (Resample)</h3><p>Input: Finite event-variable bigraph <span class="math inline">\((\mathcal{B} \cup \mathcal{X}, E)\)</span>, procedure for sampling from each random variable <span class="math inline">\(X \in \mathcal{X}\)</span></br>Output: Assignments <span class="math inline">\(v = (v_X)_{X \in \mathcal{X}}\)</span> of variables such that no event <span class="math inline">\(B \in \mathcal{B}\)</span> occurs</p><ol style=list-style-type:decimal><li>Randomly sample assignment <span class="math inline">\(v_X \sim X\)</span> for each variable <span class="math inline">\(X \in \mathcal{X}\)</span></li><li>While <span class="math inline">\(\exists\)</span> event <span class="math inline">\(B \in \mathcal{B}\)</span> which occurs under assignments <span class="math inline">\(v\)</span></li><li>     Resample <span class="math inline">\(v_X \sim X\)</span> for each variable <span class="math inline">\(X \in \operatorname{vbl}(B)\)</span></li><li>Return <span class="math inline">\(v\)</span></li></ol></div><p>Of course, assumptions on the events and their structure is necessary for <strong>Resample</strong> to run efficiently (or even to just terminate). Initially, Moser and Tardos proved good performance under asymmetric LLL conditions<a href=#fn11 class=footnote-ref id=fnref11><sup>11</sup></a>.</p><div class=Theorem><h3 id=running-time-of-resample>Theorem (Running time of Resample)</h3><p>If there exists some <span class="math inline">\(x \in (0,1)^{\mathcal{B}}\)</span> such that
<span class="math display">\[
\Pr[B] \leq x_B \prod_{C : \operatorname{vbl}(B) \cap \operatorname{vbl}(C) \neq \emptyset} (1 - x_C) \quad \forall B \in \mathcal{B},
\]</span>
then <strong>Resample</strong> finds an assignment of variables such that no event occurs in at most
<span class="math display">\[
\sum_{B \in \mathcal{B}} \frac{x_B}{1 - x_B}
\]</span>
expected resampling steps.</p></div></div><div id=entropy-compression class="section level2"><h2>Entropy Compression</h2><p>In a 2009 <a href=https://terrytao.wordpress.com/2009/08/05/mosers-entropy-compression-argument/>blog post</a>, Terence Tao presented a particularly elegant analysis of this algorithm, inspired by a talk Moser gave at STOC. His write-up coined the term <em>entropy compression</em>, a clever principle for bounding the running time of an algorithm using a reduction from data compression. To describe this method, we first introduce some fundamental definitions and results from information theory. See David MacKay’s <a href=http://www.inference.org.uk/mackay/itila/book.html>textbook</a> for a more comprehensive reference.</p><div class=Definition><h3 id=information-content>Definition (Information Content)</h3><p>The <strong>information content</strong>, or surprisal, of an event <span class="math inline">\(E\)</span> is given by
<span class="math display">\[
I(E) = -\log \Pr(E),
\]</span>
with the logarithm taken in base 2.</p></div><p>This notion captures the fact that we learn more from the occurence of a rare event than a frequent event.</p><div class=Definition><h3 id=entropy>Definition (Entropy)</h3><p>The <strong>(Shannon) entropy</strong> of a discrete random variable <span class="math inline">\(X\)</span> with finite support <span class="math inline">\(\mathcal{X}\)</span> is defined as
<span class="math display">\[
H(X) = \mathbb{E}_{x \sim X} I(X = x) = - \sum_{x \in \mathcal{X}} \Pr[X = x] \log(\Pr[X = x]).
\]</span></p></div><p>Entropy captures the expected amount of information obtained from measuring a random variable. Our exact choices of functions for <span class="math inline">\(I\)</span> and <span class="math inline">\(H\)</span> are supported by an elegant connection to coding theory.</p><div class=Definition><h3 id=uniquely-decodable-codes>Definition (Uniquely Decodable Codes)</h3><p>For two alphabets (i.e. sets of symbols) <span class="math inline">\(\Sigma_1\)</span> and <span class="math inline">\(\Sigma_2\)</span>, a <strong>code</strong> is function <span class="math inline">\(C: \Sigma_1 \to \Sigma_2^*\)</span> mapping each symbol of <span class="math inline">\(\Sigma_1\)</span> to a string of symbols over <span class="math inline">\(\Sigma_2\)</span>. The <strong>extension</strong> of <span class="math inline">\(C\)</span> is the natural homomorphism it induces from <span class="math inline">\(\Sigma_1^*\)</span> to <span class="math inline">\(\Sigma_2^*\)</span>, mapping each <span class="math inline">\(s_1s_2 \dots s_n \in \Sigma_1^*\)</span> to <span class="math inline">\(C(s_1)C(s_2) \dots C(s_n) \in \Sigma_2^*\)</span>. A code is <strong>uniquely decodable</strong> if its extension is injective.</p></div><p>The following result establishes the limits of noiseless data compression in terms of entropy.</p><div class=Theorem><h3 id=shannons-source-coding-theorem-simplified>Theorem (Shannon’s Source Coding Theorem, Simplified)</h3><p>For a discrete random variable <span class="math inline">\(X\)</span> with finite support <span class="math inline">\(\mathcal{X}\)</span>, let <span class="math inline">\(f:\mathcal{X} \to \{0,1\}^*\)</span> be a uniquely decodable code. Then, we can bound the expected length of the codeword <span class="math inline">\(f(X)\)</span> from below by the entropy of <span class="math inline">\(X\)</span>, i.e.
<span class="math display">\[
\mathbb{E}|f(X)| \geq H(X).
\]</span></p></div><p>Of particular interest to computer scientists is the following example.</p><div class=Example><h3>Example</h3><p>In particular, if <span class="math inline">\(X \sim U(\{0,1\}^r)\)</span> is an <span class="math inline">\(r\)</span>-bit string selected uniformly at random, then
<span class="math display">\[\begin{align*}
\mathbb{E}|f(X)| \geq H(X) &= - \sum_{x \in \{0,1\}^r} \Pr[X = x] \log(\Pr[X = x])\\
&= -\log(1/2^r) = r
\end{align*}\]</span>
for any uniquely decodable <span class="math inline">\(f:\{0,1\}^r \to \{0,1\}^*\)</span>. Essentially, this means that <span class="math inline">\(r\)</span> random bits cannot be noiselessly compressed into fewer than <span class="math inline">\(r\)</span> bits in expectation.</p></div><p>Finally, we are prepared to outline Tao’s entropy compression principle.</p><div class=Theorem><h3 id=entropy-compression-1>Theorem (Entropy Compression)</h3><p>Consider an algorithm <span class="math inline">\(\mathcal{A}\)</span> with access to a stream of bits sampled independently and uniformly at random. Suppose that <span class="math inline">\(\mathcal{A}\)</span> proceeds though a sequence of steps <span class="math inline">\(t = 0,1,\dots\)</span> until (potentially) terminating, such that the following hold for any input:</p><ul><li><span class="math inline">\(\mathcal{A}\)</span> can be modified to maintain a bit string <span class="math inline">\(L_t\)</span> logging its history after each step <span class="math inline">\(t\)</span>, such that the random bits read so far can be recovered from <span class="math inline">\(L_t\)</span>;<ul><li>The computational cost (or even computability) of this history log modification is irrelevant.</li><li>The precise notion of recovery is a bit nuanced<a href=#fn12 class=footnote-ref id=fnref12><sup>12</sup></a> – we will elaborate within the proof.</li></ul></li><li><span class="math inline">\(L_t\)</span> has length at most <span class="math inline">\(\ell_0 + t\Delta \ell\)</span> after step <span class="math inline">\(t\)</span>;</li><li><span class="math inline">\(r_0 + t\Delta r\)</span> random bits have been read after step <span class="math inline">\(t\)</span>.</li></ul><p>If <span class="math inline">\(\Delta \ell &lt; \Delta r\)</span>, then the step after which <span class="math inline">\(\mathcal{A}\)</span> terminates is at most
<span class="math display">\[
\frac{\ell_0 - r_0}{\Delta r - \Delta \ell}
\]</span>
in expectation.</p></div><div class=Proof><h3>Proof</h3><p>Fix any input to the algorithm. For each (<span class="math inline">\(r_0 + t\Delta r\)</span>)-bit string <span class="math inline">\(y\)</span>, sufficient to run the algorithm up to step <span class="math inline">\(t\)</span>, let <span class="math inline">\(s = s(y) \leq t\)</span> denote the step at which the algorithm terminates when its random stream is replaced by the fixed string <span class="math inline">\(y\)</span>. Then, we denote the final history log by <span class="math inline">\(L_s(y)\)</span> and let <span class="math inline">\(z(y)\)</span> be the suffix of <span class="math inline">\(y\)</span> consisting of the <span class="math inline">\((t - s)\Delta r\)</span> bits unused due to early termination. Making the notion of recovery for the history logging protocol precise, we require that the code <span class="math inline">\(f:\{0,1\}^{r_0 + t\Delta r} \to \{0,1\}^*\)</span> defined by the concatenation
<span class="math display">\[
f(y) = L_s(y) \circ z(y)
\]</span>
is uniquely decodable, for all <span class="math inline">\(t\)</span>. This is a slightly technical condition, but it will not be an obstacle for our analysis of Moser’s algorithm. Now, supplying random bits <span class="math inline">\(Y \sim U(\{0,1\}^{r_0 + t\Delta r})\)</span> to the algorithm, resulting in a run to step <span class="math inline">\(S = s(Y)\)</span>, we find that
<span class="math display">\[
|f(Y)| = |L_S(Y)| + (t - S)\Delta r \leq \ell_0 + S \Delta \ell + (t - S)\Delta r.
\]</span>
Next, we use the source coding bound of <span class="math inline">\(\mathbb{E}|f(Y)| \geq H(Y)\)</span> and rearrange to obtain
<span class="math display">\[
\mathbb{E}[S] \leq \frac{\ell_0 - r_0}{\Delta r - \Delta \ell}.
\]</span>
Since <span class="math inline">\(\mathcal{A}\)</span> always completes its zeroth step, source coding also requires that <span class="math inline">\(\ell_0 \geq r_0\)</span>, ensuring that the previous bound is always sensical (non-negative). Finally, if <span class="math inline">\(T\)</span> denotes the terminating step of the algorithm when provided with a stream of random bits (with the convention that <span class="math inline">\(T=\infty\)</span> when the algorithm fails to halt), we see that <span class="math inline">\(S = \min\{t,T\}\)</span>. Since our bound on <span class="math inline">\(\mathbb{E}[S]\)</span> is independent of <span class="math inline">\(t\)</span>, it also applies to <span class="math inline">\(\mathbb{E}[T] = \lim_{t \to \infty}\mathbb{E}[S]\)</span>, as desired.</p></div><p>Essentially, if our algorithm is logging fewer bits than it reads at each step, it cannot progress too far in expectation without acting as an impossibly good compression protocol. Note that the identity protocol, which simply logs the random bits read so far without modification, satisfies our unique decodability requirement and can be appended to any algorithm. In this case, <span class="math inline">\(\Delta \ell = \Delta r\)</span>, and the entropy compression bound means that an algorithm admitting a better protocol cannot continue indefinitely.</p><div class=Remark><h3>Remark</h3><p>Alternatively, one can formalize this analysis using <a href=https://en.wikipedia.org/wiki/Kolmogorov_complexity>Kolmogorov complexity</a> to get a bound with high probability, as described in this <a href=https://blog.computationalcomplexity.org/2009/06/kolmogorov-complexity-proof-of-lov.html>blog post</a> by Lance Fortnow.</p></div></div><div id=analysis-of-mosers-algorithm class="section level2"><h2>Analysis of Moser’s Algorithm</h2><p>Now, we will apply our new found principle to analyze Moser’s algorithm. For conciseness, we will restrict ourselves to the <span class="math inline">\(k\)</span>-SAT example and consider a slightly modified procedure. Recall that we are requiring <span class="math inline">\(k\)</span>-CNF formulas to have exactly <span class="math inline">\(k\)</span> distinct variables in each clause.</p><div class=Algorithm><h3 id=resample2>Algorithm (Resample2)</h3><p>Input: <span class="math inline">\(k\)</span>-CNF formula <span class="math inline">\(\varphi\)</span><br>Output: Assignments <span class="math inline">\(x \leftarrow\)</span> <strong>solve</strong>(<span class="math inline">\(\varphi\)</span>) such that <span class="math inline">\(\varphi(x)\)</span> holds</p><ol style=list-style-type:decimal><li><strong>solve</strong>(<span class="math inline">\(\varphi\)</span>)</li><li>     Randomly assign each <span class="math inline">\(x_i\)</span> true or false with equal probability</li><li>     While <span class="math inline">\(\exists\)</span> unsatisfied clause <span class="math inline">\(C\)</span></li><li>         x <span class="math inline">\(\leftarrow\)</span> <strong>fix</strong>(<span class="math inline">\(x, C, \varphi\)</span>)</li><li>     Return <span class="math inline">\(x\)</span></li><li><strong>fix</strong>(<span class="math inline">\(x, C, \varphi\)</span>)</li><li>     Randomly resample each <span class="math inline">\(x_i \in C\)</span></li><li>     While <span class="math inline">\(\exists\)</span> unsatisfied clause <span class="math inline">\(D\)</span> sharing a variable with <span class="math inline">\(C\)</span></li><li>         x <span class="math inline">\(\leftarrow\)</span> <strong>fix</strong>(<span class="math inline">\(x, \varphi, D\)</span>)</li><li>     Return <span class="math inline">\(x\)</span></li></ol></div><p>This algorithm is very similar in spirit to <strong>Resample</strong>, with the added recursive step of fixing nearby clauses before continuing to other unsatisfied clauses. We will prove efficient running time bounds for this algorithm assuming slightly relaxed symmetric LLL conditions, under a model of computation where sampling a single random bit takes <span class="math inline">\(O(1)\)</span> time.</p><div class=Theorem><h3 id=running-time-of-resample2>Theorem (Running Time of Resample2)</h3><p>Let <span class="math inline">\(\varphi\)</span> be a <span class="math inline">\(k\)</span>-CNF formula with <span class="math inline">\(n\)</span> variables and <span class="math inline">\(m\)</span> clauses. If each clause of <span class="math inline">\(\varphi\)</span> shares variables with at most <span class="math inline">\(2^{k - C}\)</span> other clauses, then <strong>Resample2</strong> runs in <span class="math inline">\(O(n + mk)\)</span> expected time, where <span class="math inline">\(C\)</span> is an absolute constant.</p></div><div class=Remark><h3>Remark</h3><p>Note that <span class="math inline">\(C = \log_2(e)\)</span> would match the symmetric LLL conditions exactly.</p></div><div class=Proof><h3>Proof</h3><p>To apply entropy compression, we must first divide each run of <strong>Resample2</strong> into steps. We let the initial assignments of <strong>solve</strong> comprise step 0 and let each clause resampling by <strong>fix</strong> consistitute an additional step. Our key observation is that only one assignment of the relevant <span class="math inline">\(k\)</span> variables can cause a <span class="math inline">\(k\)</span>-CNF clause to be violated and require fixing. Thus, given a stack trace of an algorithm run, we can work backwards from the final string <span class="math inline">\(x\)</span> to recover the sampled random bits. For example, suppose that <span class="math inline">\(x_1 \lor \neg x_2 \lor x_3\)</span> was the last clause of a 3-CNF formula to be fixed by <strong>Resample2</strong>. Then, the final assignments of these variables give the last three bits sampled, and their previous assignments must have been (False, True, False).</p><p>Now, we use the dependency bound of <span class="math inline">\(d \leq 2^{k-C}\)</span> to construct an efficient history logging protocol. Every history log will begin with <span class="math inline">\(n\)</span> bits specifying the current variable assignment. Then, we mark the clauses which were fixed by non-recursive calls from <strong>solve</strong>. Since there are <span class="math inline">\(m\)</span> clauses in total, such a subset can be described by <span class="math inline">\(m\)</span> bits, with each bit specifying the membership status of a particular clause. Next, for each clause, we fix an ordering of the <span class="math inline">\(\leq d + 1\)</span> clauses it shares variables with (including itself). Then, for the clauses of recursive calls to <strong>fix</strong>, we can simply log their index with respect to the “parent” clause which called them, requiring at most <span class="math inline">\(\lceil \log(d + 1) \rceil + O(1)\)</span> bits (with constant <span class="math inline">\(O(1)\)</span> information used to keep track of the recursion stack). Reading through such a log as part of a larger string, one can determine when the log has terminated and how many steps were completed by the run it encodes, so it is simple enough to show that our unique decodability condition holds.</p><p>Thus, if the algorithm has called <strong>fix</strong> <span class="math inline">\(t\)</span> times, our log has length at most <span class="math inline">\(n + m + t(\log(d) + O(1))\)</span>. However, we have sampled <span class="math inline">\(n\)</span> bits for the initial assignments and <span class="math inline">\(k\)</span> for each call to <strong>fix</strong>, giving a total of random <span class="math inline">\(n + kt\)</span> bits. By the dependency bound, the history log “write rate” of <span class="math inline">\(\log(d) + O(1) = k - C + O(1)\)</span> bits per step is less than the random bit “read rate” of <span class="math inline">\(k\)</span> bits per step, for sufficiently large <span class="math inline">\(C\)</span>. Thus, our entropy compression principle states that the number of calls to <strong>fix</strong> is at most <span class="math inline">\(O(n + m - n) = O(m)\)</span> in expectation. The total number of random bits sampled by <strong>Resample2</strong> is therefore <span class="math inline">\(O(n + mk)\)</span> in expectation, as desired.</p></div><p>Many lower bounds in theoretical computer science are proven using information theory, but Moser was the first, to my knowledge, to provide an upper bound using such methods. The key ingredient of his original proof was that of a <em>witness tree</em>, similar in spirit to our history log above. Although this technique was quite novel at the time, Tao observed that it could framed in a more familiar light. Algorithm analysis often exploits monotonic properties that consistently increase over the course of an algorithm. If one can ensure that there is a constant increase at each step and that the property is bounded from above, then this gives an upper bound on the number of steps taken by the algorithm (common examples of monotonic properties include cardinalities, weights, densities, and energies). In this case, we essentially<a href=#fn13 class=footnote-ref id=fnref13><sup>13</sup></a> have that the ratio of entropy to output plus history log length is bounded by one and increases by a constant amount with each (expected) step, ensuring termination. With this interpretation, “entropy compression” is a very natural description of our analysis method.</p><div class=Remark><h3>Remark</h3><p>As a final aside, I’d like to promote the excellent <a href=https://www.budapestsemesters.com/>Budapest Semesters in Mathematics</a> program, where I first learned about the LLL in András Gyárfás’s class on the combinatorics of finite set systems.</p></div></div><div class=footnotes><hr><ol><li id=fn1><p>Paul Erdős and László Lovász. <a href=http://web.cs.elte.hu/~lovasz/scans/LocalLem.pdf>“Problems and Results on 3-Chromatic Hypergraphs and Some Related Questions”</a>.<a href=#fnref1 class=footnote-back>↩</a></p></li><li id=fn2><p>Michael Molloy. <a href=https://link.springer.com/chapter/10.1007/978-3-662-12788-9_1>“The Probabilistic Method”</a>.<a href=#fnref2 class=footnote-back>↩</a></p></li><li id=fn3><p>Jószef Beck. <a href=https://doi.org/10.1002/rsa.3240020402>“An Algorithmic Approach to the Lovász Local Lemma”</a>.<a href=#fnref3 class=footnote-back>↩</a></p></li><li id=fn4><p>Robin A. Moser. <a href=https://arxiv.org/abs/0810.4812>“A Constructive Proof of the Lovász Local Lemma”</a>.<a href=#fnref4 class=footnote-back>↩</a></p></li><li id=fn5><p>Robin A. Moser and Gábor Tardos. <a href=https://arxiv.org/abs/0903.0544>“A Constructive Proof of the General Lovász Local Lemma”</a>.<a href=#fnref5 class=footnote-back>↩</a></p></li><li id=fn6><p>Noga Alon and Joel H. Spencer. <a href=https://doi.org/10.1002/9780470277331>“The Probabilistic Method”</a>.<a href=#fnref6 class=footnote-back>↩</a></p></li><li id=fn7><p>Paul Erdős and László Lovász. <a href=http://web.cs.elte.hu/~lovasz/scans/LocalLem.pdf>“Problems and Results on 3-Chromatic Hypergraphs and Some Related Questions”</a>.<a href=#fnref7 class=footnote-back>↩</a></p></li><li id=fn8><p>James B. Shearer. <a href=https://doi.org/10.1007/BF02579368>“On a Problem of Spencer”</a>.<a href=#fnref8 class=footnote-back>↩</a></p></li><li id=fn9><p>Robin A. Moser. <a href=https://arxiv.org/abs/0810.4812>“A Constructive Proof of the Lovász Local Lemma”</a>.<a href=#fnref9 class=footnote-back>↩</a></p></li><li id=fn10><p>Robin A. Moser and Gábor Tardos. <a href=https://arxiv.org/abs/0903.0544>“A Constructive Proof of the General Lovász Local Lemma”</a>.<a href=#fnref10 class=footnote-back>↩</a></p></li><li id=fn11><p>Robin A. Moser and Gábor Tardos. <a href=https://arxiv.org/abs/0903.0544>“A Constructive Proof of the General Lovász Local Lemma”</a>.<a href=#fnref11 class=footnote-back>↩</a></p></li><li id=fn12><p>In fact, I was confused enough to ask about it on Math Overflow, where Tao answered my (misguided) <a href=https://mathoverflow.net/questions/347787/formalizing-entropy-compression-as-used-to-constructify-the-lov%c3%a1sz-local-lemma>question</a> himself!<a href=#fnref12 class=footnote-back>↩</a></p></li><li id=fn13><p>Formalizing this in the proof required some care, capping the length of the random stream, but the description is morally correct.<a href=#fnref13 class=footnote-back>↩</a></p></li></ol></div></div></article><section id=comments><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){var inIFrame=function(){var iframe=true;try{iframe=window.self!==window.top;}catch(e){}
return iframe;};if(inIFrame())return;var d=document,s=d.createElement('script');s.src='//sloan.disqus.com/embed.js';s.async=true;s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></section></main><footer class=footer><ul class=footer-links><li><a href=../../../../index.xml type=application/rss+xml target=_blank>RSS feed</a></li><li><a href=https://gohugo.io/ class=footer-links-kudos>Made with <img src=../../../../images/hugo-logo.png alt="Img link to Hugo website" width=22 height=22></a></li></ul></footer></div><script type=text/javascript src=../../../../js/responsive-tables.js></script><script src=../../../../js/math-code.js></script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-57859205-2','auto');ga('send','pageview');}</script></body></html>